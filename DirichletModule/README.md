<!-- Uses markdown syntax for neat display at github. This is the most important thing to your user. Be not afraid that
	you are too long-winded. If you tell someone what the Battacharyya distance is, they probably will appreciate
	that even if they already know. Be also clear about its complexity, say if it is exponential in time or the 
	number of pixels for example. 

	Tips on syntax:
	
	Use pictures:
	  ![picture](https://raw.github.com/git_username/git_repos/master/module_name/some_doc_folder/picture.jpg)

	Use math notation (http://stackoverflow.com/questions/11256433):
	- Experiment on http://latex.codecogs.com/gif.latex?c=\sqrt{E/m} to check your equation
	- Encode the math part c=\sqrt{E/m} on http://www.url-encode-decode.com/urlencode
	- And write it in markdown syntax as:
	   ![equation](http://latex.codecogs.com/gif.latex?c%3D%5Csqrt%7BE%2Fm%7D)
-->

# DirichletModule

## What does it do?

The DirichletModule implements a Dirichlet mixture model. It can be used to cluster data. This type of clustering
method is a so-called nonparametric Bayesian method. Different from for example k-means clustering it does not 
require a number of clusters to be predefined in advance. The method itself figures out how many clusters are likely
to have generated the data. Of course, this doesn't mean that there is no prior knowledge involved. 

In the current algorithm the data is assumed to be generated by Gaussians. These Gaussians are generated by another
level of random variables, namely through a Normal-Inverse Wishart (NIW) distribution. The NIW distribution generates
both the means as well as the covariance matrices for the Gaussians. The NIW is parametrized by four parameters:

* kappa
* mu
* nu
* lambda

See [NIW on Wikipedia](http://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution) for more information.

What is not specific to this algorithm, but is shared by all Dirichlet mixture models, is the so-called "dispersion
factor" or "concentration parameter". This factor specifies the distribution of the number of items over the groups. 
Often this factor is called alpha. The distribution that results is quite particular: it leads to a typical
"rich get richer" behaviour. There are groups with a lot of items, while there are also groups with only a few items.
This allows for robust behaviour, outliers can be captured by groups with only a single item for example. Note, that
the distribution has only exponentional tails. If there are many, many small groups, you might need instead an
underlying Pitman-Yor process which models power-law tails rather than an underlying Dirichlet Process.

See the [Dirichlet Process on Wikipedia](http://en.wikipedia.org/wiki/Dirichlet_process) for more information on the 
Dirichlet Process itself.

## What is the status?

The implementation of the standard backend is finished. This uses data from the `/data` folder. To get data using
the ports with for example the YARP middleware has not been tested yet.

The algorithm that has been implemented:

* Neal's algorithm 1.

This algorithm assumes a conjugate prior. It might be the case that I will find to implement algorithm 4 as well,
which is a collapsed Gibbs sampler. The latter leads to much faster mixing that the current algorithm. Currently,
however, it is reasonable to assume that I will first implement an algorithm that doesn't require conjugacy, for
example algorithm 5.

## Can I see some results?

The initial values for the clusters are determined randomly. The covariances are not visualized (yet). 

<img src="//github.com/mrquincle/aim_modules/raw/master/DirichletModule/docs/images/init.jpg" width="320" height="240" 
	title="Initial cluster" hspace="10"/>
<img src="//github.com/mrquincle/aim_modules/raw/master/DirichletModule/docs/images/clusters.jpg" width="320" height="240" 
	title="After 1000 iterators" hspace="10"/>

If we run Gibbs sampling for 1000 iterators. Note that this image concerns a single Gibbs step. It still needs to be
thinned (because it is a Markov chain), burned in (skip first samples) and averaged (over samples to get expected
values for the random variables involved: the clusters).

## How fast is it?

The DirichletModule is not implemented with speed in mind. Although some decisions have been made, such as the use of
the Eigen library, there is been no optimization of the algorithm at hand. For example, the parameters belonging to 
the clusters are dynamically allocated and reallocated. 

The reason that no optimization of data structures is attempted, is because the current algorithm itself is not 
converging particularly fast. This is Neal's algorithm 1. 

## How to install?

Follow the instructions on the [AIM website](http://dobots.github.com/aim/). 

## Where can I read more?

* [AIM website](http://dobots.github.com/aim-bzr/) 
* Escobar1994               Estimating Normal Means with a Dirichlet Process Prior (1994) Escobar                      
* Neal2000                  Markov Chain Sampling Methods for Dirichlet Process Mixture Models (2000) Neal             
* Murphy2007                Conjugate Bayesian analysis of the Gaussian distribution (2007) Murphy  

## Copyrights

The copyrights of this module (2013) belong to:

- Author: Anne C. van Rossum
- Distributed Organisms B.V.

