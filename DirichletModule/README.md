<!-- Uses markdown syntax for neat display at github. This is the most important thing to your user. Be not afraid that
	you are too long-winded. If you tell someone what the Battacharyya distance is, they probably will appreciate
	that even if they already know. Be also clear about its complexity, say if it is exponential in time or the 
	number of pixels for example. 

	Tips on syntax:
	
	Use pictures:
	  ![picture](https://raw.github.com/git_username/git_repos/master/module_name/some_doc_folder/picture.jpg)

	Use math notation (http://stackoverflow.com/questions/11256433):
	- Experiment on http://latex.codecogs.com/gif.latex?c=\sqrt{E/m} to check your equation
	- Encode the math part c=\sqrt{E/m} on http://www.url-encode-decode.com/urlencode
	- And write it in markdown syntax as:
	   ![equation](http://latex.codecogs.com/gif.latex?c%3D%5Csqrt%7BE%2Fm%7D)
-->

# DirichletModule

## What does it do?

The DirichletModule implements a Dirichlet mixture model. It can be used to cluster data. This type of clustering
method is a so-called nonparametric Bayesian method. Different from for example k-means clustering it does not 
require a number of clusters to be predefined in advance. The method itself figures out how many clusters are likely
to have generated the data. Of course, this doesn't mean that there is no prior knowledge involved. 

In the current algorithm the data is assumed to be generated by Gaussians. These Gaussians are generated by another
level of random variables, namely through a Normal-Inverse Wishart (NIW) distribution. The NIW distribution generates
both the means as well as the covariance matrices for the Gaussians. The NIW is parametrized by four parameters:

* kappa
* mu
* nu
* lambda

See [NIW on Wikipedia](http://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution) for more information.

What is not specific to this algorithm, but is shared by all Dirichlet mixture models, is the so-called "dispersion
factor" or "concentration parameter". This factor specifies the distribution of the number of items over the groups. 
Often this factor is called alpha. The distribution that results is quite particular: it leads to a typical
"rich get richer" behaviour. There are groups with a lot of items, while there are also groups with only a few items.
This allows for robust behaviour, outliers can be captured by groups with only a single item for example. Note, that
the distribution has only exponentional tails. If there are many, many small groups, you might need instead an
underlying Pitman-Yor process which models power-law tails rather than an underlying Dirichlet Process.

See the [Dirichlet Process on Wikipedia](http://en.wikipedia.org/wiki/Dirichlet_process) for more information on the 
Dirichlet Process itself.

## What is the status?

The implementation of the standard backend is finished. This uses data from the `/data` folder. To get data using
the ports with for example the YARP middleware has not been tested yet.

The algorithms that have been implemented:

* Neal's algorithm 1: Gibbs sampling with conjugate priors
* Neal's algorithm 5: Metropolis-Hastings for nonconjugate priors.

Algorithm 1 assumes a conjugate prior. It might be the case that I will find to implement algorithm 4 as well,
which is a collapsed Gibbs sampler. The latter leads to much faster mixing that the current algorithm. 

Algorithm 5 does not assume conjugacy. Although again the data is assumed to follow a Gaussian per class, there is no
conjugate prior defined. This means that this algorithm is easy to adjust to other forms of distributions. This, 
however, is not done yet.

Algorithm 1 is finished. Algorithm 5 is still under development. Do not use it! 

## Can I see some results?

The initial values for the clusters are determined randomly. The covariances are not visualized (yet). 

<img src="https://github.com/mrquincle/aim_modules/raw/master/DirichletModule/docs/images/init.jpg" width="320" height="240" 
	title="Initial cluster" hspace="10"/>
<img src="https://github.com/mrquincle/aim_modules/raw/master/DirichletModule/docs/images/clusters.jpg" width="320" height="240" 
	title="After 2000 iterations" hspace="10"/>

In the images above at the left we start clustering, at the right we are at the 2000th Gibbs sampling step. Note that 
this image visualizes a single Gibbs step. To get a nice posterior you will need to consider multiple results. It needs 
to be thinned (because it is a Markov chain), burned-in (skip first samples) and averaged (over samples to get expected
values for the random variables involved: the clusters).

The visualization scripts use `octave` and can be found in the `/scripts` directory. Covariances are displayed with
ellipses.

## How fast is it?

The DirichletModule is not implemented with speed in mind. Although some decisions have been made, such as the use of
the Eigen library, there is been no optimization of the algorithm at hand. For example, the parameters belonging to 
the clusters are dynamically allocated and reallocated. 

The reason that no optimization of data structures is attempted, is because the current algorithm itself is not 
converging particularly fast. This is Neal's algorithm 1. 

## How to install?

Follow the instructions on the [AIM website](http://dobots.github.com/aim/). 

## Where can I read more?

* [AIM website](http://dobots.github.com/aim-bzr/) 
* Estimating Normal Means with a Dirichlet Process Prior (1994) Escobar                      
* Markov Chain Sampling Methods for Dirichlet Process Mixture Models (2000) Neal             
* Conjugate Bayesian analysis of the Gaussian distribution (2007) Murphy  

For a detailed derivation of some of the algorithms, see also my blog:

* http://www.annevanrossum.com/blog/2015/03/03/sampling-of-dirichlet-process/

## Copyrights

The copyrights of this module (2015) belong to:

- Author: Anne C. van Rossum
- Distributed Organisms B.V. (https://dobots.nl)
- License: LPGPv3, Apache, MIT (your choice)
